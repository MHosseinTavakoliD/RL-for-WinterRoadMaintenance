{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T15:57:50.508975100Z",
     "start_time": "2024-03-11T15:57:50.288035400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Virtual Environment\n",
    "# All the units are in SI system\n",
    "import numpy as np\n",
    "# \n",
    "\n",
    "# Virtual Environment\n",
    "# All the units are in SI system\n",
    "import numpy as np\n",
    "# \n",
    "\n",
    "def Matrial_on_the_road(Air_temp, Suraface_temp, Precipitation, wind_vel,initial_Snow, initial_water, initial_Salt, initial_ice, plowing):\n",
    "    # General assumptions:\n",
    "    Dataset_hour_inc = 1 # *3 had applied in the dataset\n",
    "    f_t = 0.9 #times when the road is covered with moving vehicle\n",
    "    g_t = 1 - f_t\n",
    "    \n",
    "    # print (\"Air_temp, Suraface_temp, Precipitation, wind_vel,initial_Snow, initial_water, initial_Salt, initial_ice, plowing\")\n",
    "    # print (Air_temp, Suraface_temp, Precipitation, wind_vel,initial_Snow, initial_water, initial_Salt, initial_ice, plowing)\n",
    "    \n",
    "    # convert Precipitation to snow or rain based on the air temp\n",
    "    if Air_temp < .5:\n",
    "        Snowfall_amount = Precipitation\n",
    "        Rainfall_amount = 0\n",
    "    else:\n",
    "        Rainfall_amount = Precipitation\n",
    "        Snowfall_amount = 0\n",
    "    \n",
    "    ###### Heat Ballance ##########################################################################################################\n",
    "    # Q_csp : Flux of pavement heat\n",
    "    V_wis = initial_ice + initial_Snow + Snowfall_amount # Volume of WIS layer (m3*m-2 = m) depth of material on the road\n",
    "    V_ps = 50e-3 # thikness of pavement surface, generally 25 to 55 mm, here we assume 50mm\n",
    "    Lambda_wis = 0.8 # thermal conductivity of WIS layer. Assume that is compacted snow \n",
    "    Lambda_P = 1.5 # Thermal conductivity of pavemetn. 0.8 to 2\n",
    "    T_wis = (Air_temp + Suraface_temp)/2 #Temp of wis layer: assumption \n",
    "    Q_csp = 1/((V_wis/(2*Lambda_wis))+(V_ps/(2*Lambda_P)))*(Suraface_temp - T_wis)\n",
    "    \n",
    "     # Q_rn : Flux of net radient heat\n",
    "    q_rld = 20 #Since we are modeling the winter stroms, the sky radiation shouldn't be high 0-200\n",
    "    q_rlu = 0.97 * 5.64e-8 * (T_wis + 273.15)**4\n",
    "    q_rsd = 20 #Since we are modeling the winter stroms, the sky radiation shouldn't be high 0-300\n",
    "    q_rsu = 0.3 * q_rsd\n",
    "    Q_rn = f_t*q_rld + q_rlu + f_t*q_rsd - q_rsu\n",
    "    \n",
    "    # Q_sn Flux of net radiant heat\n",
    "    q_sa = (10e4 * wind_vel**0.7 +2.2)*(T_wis - Air_temp)#snesable heat flux due to wind\n",
    "    q_sf = (4.184* (initial_water + Rainfall_amount) + 2.108 * (initial_ice  + initial_Snow + Snowfall_amount))# Sensable heat flux due to rainfall and snowfall\n",
    "    q_sr = 0 #Sensible heat flux of drainage due to road gradient assume = 0\n",
    "    q_sv = 4.184 * T_wis * 1000 * 0.5 * 0.005/3600 * 0.15#sensible heat flux of water dispersion due to passing vehicle\n",
    "    Q_sn = q_sa + f_t*q_sf + q_sr + g_t * q_sv\n",
    "    \n",
    "    # Q_ln flux of net latent heat\n",
    "    # m_wi * L_wi is not considered here, we will change the q_net relation with M_wi later\n",
    "    m_il = 0 # sublimation flux\n",
    "    L_i = 2838 #kJkg-1 latent heat of sublimation\n",
    "    m_wl = 0 # flux of evaporation and condensation\n",
    "    L_w = 2260 #kJkg-1 latent heat of evaporation and condensation\n",
    "    m_sl = 3.34e-5 # dissolving flux (0 - 6.67e-5) if salting\n",
    "    L_s = -66.4 # latent heat of dissolution of salt \n",
    "    Q_ln = m_il*L_i + m_wl*L_w + m_sl*L_s\n",
    "    \n",
    "    # Q_vn flux of net vehicle heat\n",
    "    Q_vn = 100 #assumption according to Fujimoto and other citations\n",
    "    \n",
    "    # Q_net\n",
    "    Q_net = Q_csp + Q_rn + Q_sn + Q_ln + Q_vn\n",
    "    #################################################################################################################################\n",
    "    #*******************************************************************************************************************************#\n",
    "    \n",
    "    ## Material creation $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    # Ice \n",
    "    Ice_final = initial_ice \n",
    "    \n",
    "    # Snowfall \n",
    "    m_snowf = Snowfall_amount * Dataset_hour_inc * 100 # 100 kg/m3\n",
    "    Snow_creat = m_snowf * f_t\n",
    "    Snow_final = Snow_creat + initial_Snow\n",
    "    # plowing  $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    if plowing == 1:\n",
    "        Snow_final *= 0.1\n",
    "        \n",
    "    # or rainfall\n",
    "    m_wf = Rainfall_amount * Dataset_hour_inc * 1000 # 1000 kg/m3\n",
    "    Rain_creat = m_wf * f_t\n",
    "    Water_final = Rain_creat + initial_water\n",
    "    # before mesuring the water to ice, we have to consider the water dispersion\n",
    "    # Water dispersion\n",
    "    # Assume we have good drainage system\n",
    "    Water_final = Water_final * 0.95\n",
    "    \n",
    "    #Salt\n",
    "    # Salt_final = initial_Salt * 0.5# Maybe need conversion\n",
    "    \n",
    "    ### Material Conversion $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    ## Snow to ice\n",
    "    Snow_To_Ice_rate = (0.1 if initial_Snow >0 else 0) # Assumption: conversion applied to initial snow \n",
    "    Ice_final += Snow_To_Ice_rate * initial_Snow\n",
    "    \n",
    "    ## water to ice or vise versa *\n",
    "    L_wi = 334 #kJkg-1 latent heat of melting and freezing\n",
    "    Water_ice_conv_mass = abs(Q_net*1000/(L_wi*999)) if initial_Snow >0 or initial_water>0 or initial_ice>0 else 0\n",
    "    # We will assume that the water freezing point is just a function of salt concentration\n",
    "    T_frezzing = -0.025 * initial_Salt - 0.5 #0.04 * initial_Salt + 1#\n",
    "    #T_frezzing = (T_frezzing -32)*5/9 \n",
    "    # print(\"TF\",T_frezzing, \"initial_Salt\", initial_Salt, \"T_wis\", T_wis  )\n",
    "    \n",
    "    if T_wis >= T_frezzing:\n",
    "        STATUS = \"Melting\"\n",
    "        # print(\"###############################Melting\")  # Melting\n",
    "        stat_sign = 1\n",
    "        Ice_final = Ice_final - min(Water_ice_conv_mass, Ice_final)  \n",
    "        Water_final = Water_final + min(Water_ice_conv_mass, Ice_final) \n",
    "    elif T_wis < T_frezzing and initial_Salt ==0:\n",
    "        STATUS = \"Freezing\"\n",
    "        # print(\"###############################Freezing\")  # Freezing\n",
    "        Ice_final = Ice_final + min(Water_ice_conv_mass, Water_final)  \n",
    "        Water_final = Water_final - min(Water_ice_conv_mass, Water_final) \n",
    "    elif T_wis < T_frezzing and initial_Salt >0:\n",
    "        STATUS = \"Freezing slowly\"\n",
    "        # print(\"###############################Freezing slowly\")  # Freezing\n",
    "        Ice_final = 0.2*(Ice_final + min(Water_ice_conv_mass, Water_final))\n",
    "        Water_final = Water_final - min(Water_ice_conv_mass, Water_final)\n",
    " \n",
    "    # Snow to water\n",
    "    if STATUS == \"Melting\":\n",
    "        Water_final = Water_final + stat_sign * Water_ice_conv_mass\n",
    "        Water_final = Water_final * 0.05 #disperssed\n",
    "        Snow_final = Snow_final - stat_sign * Water_ice_conv_mass\n",
    "        # print (\"Water_ice_conv_mass\", Water_ice_conv_mass)\n",
    "    ## Material dispersion $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "    # Salt dispersion: depands on STATUS\n",
    "    if STATUS == \"Melting\":\n",
    "        Salt_disp_rate = 0.9\n",
    "    if STATUS == \"Freezing\" or STATUS == \"Freezing slowly\":\n",
    "        Salt_disp_rate = 0.98\n",
    "    Salt_final = initial_Salt * Salt_disp_rate\n",
    "    \n",
    "    \n",
    "    # Check each variable and set to zero if negative\n",
    "    Water_final = max(Water_final, 0)\n",
    "    Salt_final = max(Salt_final, 0)\n",
    "    Ice_final = max(Ice_final, 0)\n",
    "    Snow_final = max(Snow_final, 0)\n",
    "    \n",
    "    \n",
    "    # print (\"Water_final, Salt_final, Ice_final, Snow_final\", Water_final, Salt_final, Ice_final, Snow_final)\n",
    "    return Water_final, Salt_final, Ice_final, Snow_final\n",
    "print (\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3adcae5ccd086c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T15:57:51.546764500Z",
     "start_time": "2024-03-11T15:57:50.498120900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 22:11:52.244383: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-07 22:11:52.285950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-07 22:11:52.286017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-07 22:11:52.286953: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-07 22:11:52.296187: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-07 22:11:52.297039: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-07 22:11:54.281028: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, model_path=None):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.action_list_size = action_size\n",
    "        self.memory = deque(maxlen=10000) #\n",
    "        self.gamma = 0.99  # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.999\n",
    "        self.learning_rate = 0.00001 #<<<<<<<<<< \n",
    "        self.Number_ofExploitation = 0\n",
    "        self.Number_ofExploration = 0\n",
    "        if model_path:\n",
    "            self.model = load_model(model_path)\n",
    "            print(\"Model loaded from\", model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "            print(\"New model initialized\")\n",
    "\n",
    "    def _build_model(self):\n",
    "        # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "         ## Input layer and first hidden layer\n",
    "        model.add(Dense(48, input_dim=total_state_size, activation='relu'))\n",
    "        ## Second hidden layer\n",
    "        # model.add(Dense(96, activation='relu'))\n",
    "        # ## Adding Dropout to prevent overfitting\n",
    "        # model.add(Dropout(0.2))\n",
    "        \n",
    "        ## Third hidden layer\n",
    "        #model.add(Dense(512, activation='relu'))\n",
    "        ## Adding Dropout to prevent overfitting\n",
    "        #model.add(Dropout(0.2))\n",
    "        \n",
    "        ## Fourth hidden layer\n",
    "        #model.add(Dense(256, activation='relu'))\n",
    "\n",
    "        model.add(Dense(124, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(124, activation='relu'))\n",
    "        model.add(Dense(self.action_list_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "\n",
    "    def act(self, state):\n",
    "        if False: #np.random.rand() <= self.epsilon:\n",
    "            # Biased action selection\n",
    "            actions_probability = [0.4, 0.2, 0.2, 0.2]  # # Example: 40% do nothing, 20% for each other action\n",
    "            self.Number_ofExploration +=1\n",
    "            return np.random.choice(np.arange(self.action_size), p=actions_probability)\n",
    "        else:\n",
    "            act_values = self.model.predict(state)\n",
    "            self.Number_ofExploitation +=1\n",
    "            return np.argmax(act_values[0])\n",
    "    \n",
    "    def predict_action(self, state):\n",
    "        \"\"\"Selects the action with the highest Q-value for the given state.\"\"\"\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "                \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            \n",
    "            \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb48c70a0c1de1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T15:57:52.666189Z",
     "start_time": "2024-03-11T15:57:51.494181300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "class RoadEnv:\n",
    "    def __init__(self, dataset, unique_window_ids):\n",
    "        self.dataset = dataset\n",
    "        self.current_state = None\n",
    "        self.unique_window_ids = unique_window_ids \n",
    "        self.current_episode_index = 0\n",
    "        # self.action_costs = {0: 0, 1: 1, 2: 6, 3: 5}\n",
    "        self.reset()\n",
    "#     def simulate_no_intervention(self, window_data):\n",
    "#         no_intervention_results = []\n",
    "#         initial_conditions = {'initial_Snow': 0, 'initial_water': 0, 'initial_Salt': 0, 'initial_ice': 0, 'plowing':0}\n",
    "#         for index, row in window_data.iterrows():\n",
    "#             # Simulate step without interventions\n",
    "#             Air_temp = row['Air TemperatureC']\n",
    "#             Surface_temp = row['Surface TemperatureC']\n",
    "#             Precipitation = row['Precipitation Intensitym/3h'] \n",
    "#             wind_vel = row['Wind Speed (act)m/s']\n",
    "#             water, salt, ice, snow = Matrial_on_the_road(Air_temp, Surface_temp, Precipitation, wind_vel, **initial_conditions)\n",
    "#             # print (\"water, salt, ice, snow\", water, salt, ice, snow)\n",
    "#             # Update initial conditions for the next step\n",
    "#             initial_conditions.update({'initial_Snow': snow, 'initial_ice': ice, 'initial_water': water, 'initial_Salt': salt, 'plowing': 0})\n",
    "            \n",
    "#             # Append results for no intervention forecast\n",
    "#             no_intervention_results.append((water, salt, ice, snow))\n",
    "#             # print(\"no_intervention_results\",no_intervention_results)\n",
    "#         return no_intervention_results\n",
    "    # def reset(self):\n",
    "    #     # Reset the environment to the initial state of a new episode\n",
    "    #     self.current_state = self.dataset.iloc[0].copy()\n",
    "    #     self.plowing_count = 0\n",
    "    #     self.salting_count = 0\n",
    "    #     \n",
    "    #     return self._get_state()\n",
    "    def reset(self, episode_index=0):\n",
    "        \n",
    "        if self.current_state is not None:\n",
    "            # Initialize the state properties if current_state is properly set\n",
    "            self.current_state['initial_Snow'] = 0\n",
    "            self.current_state['initial_water'] = 0\n",
    "            self.current_state['initial_Salt'] = 0\n",
    "            self.current_state['initial_ice'] = 0\n",
    "        # print (\"RESET self.current_state\" )\n",
    "        # print (self.current_state)\n",
    "        # Calculate the start index of the episode in the dataset\n",
    "        self.start_index = episode_index * 10  # Assuming each episode has 10 states\n",
    "        self.current_state_index = self.start_index\n",
    "        window_id = self.unique_window_ids[episode_index]\n",
    "        #print (\"window_id\", window_id)\n",
    "        # Check if the calculated start index is within the bounds of the dataset\n",
    "        if self.start_index >= 0 and self.start_index < len(self.dataset):\n",
    "            self.current_state = self.dataset.iloc[self.start_index].copy()\n",
    "        else:\n",
    "            # Fallback to the first row if the calculated index is out of bounds\n",
    "            self.current_state = self.dataset.iloc[0].copy()\n",
    "            print(f\"Warning: Episode index {episode_index} out of bounds, falling back to initial state.\")\n",
    "    \n",
    "        # Reset the action counts for the new episode\n",
    "        self.plowing_count = 0\n",
    "        self.salting_count = 0\n",
    "        \n",
    "        # Reset environment to the start of a new episode, with modifications to include no intervention forecast\n",
    "        window_id = unique_window_ids[episode_index]\n",
    "        episode_data = self.dataset[self.dataset['Window_ID'] == window_id]\n",
    "        self.Lowest_temp = min(min(episode_data['Surface TemperatureC']), min(episode_data['Air TemperatureC']))\n",
    "        #print (\"self.Lowest_temp\", self.Lowest_temp)\n",
    "        # self.no_intervention_forecast = self.simulate_no_intervention(episode_data)\n",
    "        # print (\"self.no_intervention_forecast\", self.no_intervention_forecast)\n",
    "        # Return the initial state of the new episode\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = 0#20000\n",
    "        \n",
    "        # Check if the action is allowed based on the resource limitations\n",
    "        if action == 1 and self.plowing_count >= 3:  # Plowing limit reached\n",
    "            reward = -1000  # Force 'no action'\n",
    "        elif action == 3 and self.salting_count >= 2:  # Salting limit reached\n",
    "            reward = -1000  # Force 'no action'\n",
    "        elif action == 2:  # Plowing and salting\n",
    "            if self.plowing_count >= 3 and self.salting_count >= 2:\n",
    "                reward = -2000  # If either limit is reached, force 'no action'\n",
    "            elif self.plowing_count >= 3:\n",
    "                reward = -1000\n",
    "            elif self.salting_count >= 2:\n",
    "                reward = -1000\n",
    "    \n",
    "        # Update action counters\n",
    "        if action == 1:  # Plowing\n",
    "            self.plowing_count += 1\n",
    "        elif action == 3:  # Salting\n",
    "            self.salting_count += 1\n",
    "        elif action == 2:  # Plowing and salting\n",
    "            self.plowing_count += 1\n",
    "            self.salting_count += 1\n",
    "        # Apply the chosen action to the environment and update the state\n",
    "        # print (\"action\", action)\n",
    "        \n",
    "        water, salt, ice, snow = self._apply_action(action)\n",
    "        # print (\" water, salt, ice, snow\", water, salt, ice, snow)\n",
    "        # Update the current state with the new values\n",
    "        self.current_state['initial_Snow'] = snow\n",
    "        self.current_state['initial_water'] = water\n",
    "        self.current_state['initial_Salt'] = salt\n",
    "        self.current_state['initial_ice'] = ice\n",
    "        #print (\"self.current_state &&&&&Step\", self.current_state)\n",
    "        # Calculate the reward\n",
    "        reward += self._calculate_reward(action, ice, snow, salt)\n",
    "        # print (\"reward\",reward)\n",
    "        # Move to the next row in the dataset, simulating time progression\n",
    "        \n",
    "        # Dynamically determine if it's the last state in the episode\n",
    "        current_episode_index = self.dataset.index.get_loc(self.current_state.name)\n",
    "        # print (\"current_episode_index *********************   \", current_episode_index)\n",
    "        #episode_start_index = current_episode_index - (current_episode_index % 10)  # Assuming episodes start at indices 0, 10, 20, ...\n",
    "        #next_index = current_episode_index + 1\n",
    "        #is_last_state_in_episode = (next_index % 10 == 0) or (next_index >= len(self.dataset))\n",
    "        is_last_state_in_episode = (self.current_state_index - self.start_index ==9)\n",
    "        if is_last_state_in_episode:\n",
    "            done = True\n",
    "            \n",
    "        else:\n",
    "            self.current_state_index = self.current_state_index + 1 \n",
    "            \n",
    "            next_state = self.dataset.iloc[self.current_state_index].copy()\n",
    "            for col in ['MeasureTime', 'Window_ID', 'Air TemperatureC', 'Surface TemperatureC', 'Wind Speed (act)m/s', 'Precipitation Intensitym/3h']:\n",
    "                  self.current_state[col] = next_state[col]\n",
    "            done = False\n",
    "        #print (\"self.current_state_index\", self.current_state_index, \"self.start_index\", self.start_index)\n",
    "        # print (\"SETP self.current_state\" )\n",
    "        # print (self.current_state)\n",
    "        \n",
    "        return self._get_state(), reward, done\n",
    "\n",
    "    def _get_state(self):\n",
    "        # Extract the current state from the dataset row\n",
    "        current_state_values = [self.current_state[col] for col in ['Air TemperatureC', 'Surface TemperatureC', \n",
    "                                                                     'Wind Speed (act)m/s', 'Precipitation Intensitym/3h', \n",
    "                                                                     'initial_Snow', 'initial_water', 'initial_Salt', 'initial_ice']]\n",
    "        \n",
    "        # Prepare no intervention forecast data for the entire episode\n",
    "        # Assuming no_intervention_forecast is a list of tuples for the entire episode at this point\n",
    "        #no_intervention_forecast_flat = [value for forecast in self.no_intervention_forecast for value in forecast]\n",
    "    \n",
    "        # Combine the current state values with the no intervention forecast\n",
    "        # Here, we ensure the state_with_forecast includes the no intervention data for the entire episode\n",
    "        state_ = np.array(current_state_values)# + no_intervention_forecast_flat)\n",
    "        # print (\"state_with_forecast\", state_with_forecast)\n",
    "        return state_\n",
    "\n",
    "    def _apply_action(self, action):\n",
    "        # Apply the chosen action and update the road conditions\n",
    "        # For simplicity, this function assumes the existence of a function named 'Matrial_on_the_road'\n",
    "        # that calculates the final amounts of water, salt, ice, and snow\n",
    "\n",
    "        air_temp = self.current_state['Air TemperatureC']\n",
    "        surface_temp = self.current_state['Surface TemperatureC']\n",
    "        precipitation = self.current_state['Precipitation Intensitym/3h']\n",
    "        wind_vel = self.current_state['Wind Speed (act)m/s']\n",
    "        initial_snow = self.current_state['initial_Snow']\n",
    "        initial_water = self.current_state['initial_water']\n",
    "        initial_salt = self.current_state['initial_Salt']\n",
    "        initial_ice = self.current_state['initial_ice']\n",
    "        plowing  = 0\n",
    "        #print (\"self.current_state _apply_action\", self.current_state)\n",
    "        if action == 1 or action == 2:  # Plowing or Plowing and Salting\n",
    "            plowing  = 1\n",
    "            \n",
    "\n",
    "        if action == 2 or action == 3:  # Salting or Plowing and Salting\n",
    "            # Measuring salt amount based on the Ruled-Base model\n",
    "                Salt_amount = -25 * self.Lowest_temp + 25 #-32.027 * self.Lowest_temp + 79.24\n",
    "                # print (\"Salt_amount: \", Salt_amount)\n",
    "                if Salt_amount < 0: Salt_amount = 0\n",
    "                if Salt_amount > 400: Salt_amount = 400  # here salt unit is lb/mile/lane\n",
    "                initial_salt = Salt_amount  # Assuming a constant amount of salt is used\n",
    "\n",
    "        water_final, salt_final, ice_final, snow_final = Matrial_on_the_road(\n",
    "            air_temp, surface_temp, precipitation, wind_vel,\n",
    "            initial_snow, initial_water, initial_salt, initial_ice , plowing\n",
    "        )\n",
    "        # print (f\"action {action}, water_final {water_final}, salt_final {salt_final}, ice_final {ice_final}, snow_final {snow_final}\" )\n",
    "        return water_final, salt_final, ice_final, snow_final\n",
    "\n",
    "    def _calculate_reward(self, action, ice, snow, salt):\n",
    "        # Calculate the reward based on the reduction of ice and snow, and the cost of the action\n",
    "        # cost = self.action_costs[action]\n",
    "        cost = 0\n",
    "        if action == 1: # Plowing\n",
    "            cost = 12#4.3\n",
    "        if action == 2: # Plowing and saltin\n",
    "            cost = 12 + 0.07*salt\n",
    "        if action == 3:\n",
    "            cost =  0.07*salt#4.5 + 0.07*salt\n",
    "        # Penalties\n",
    "        # Convert kg/m2 to inch of snow\n",
    "        snow_convert = snow /2.5 #100kg/m3\n",
    "        ice_convert = ice /6/2.5 #600 kg/m3\n",
    "        \n",
    "        #function correlating snow accumulation depth to decreased traffic speed\n",
    "        f_spped_snow = 0\n",
    "        if snow_convert <= 0.1: f_spped_snow = 0\n",
    "        elif snow_convert <0.5 and  snow_convert > 0.1: f_spped_snow = snow_convert * 0.2 * 100\n",
    "        elif snow_convert >= 0.5: f_spped_snow = snow_convert * 0.15 * 100\n",
    "        \n",
    "        f_spped_ice = 0\n",
    "        if ice_convert <= 0.05: f_spped_ice = 0\n",
    "        elif ice_convert > 0.05: f_spped_ice = 5 * ice_convert\n",
    "        \n",
    "        # Cost per 3h/mile of no maintence is 250$\n",
    "        Cost_speed = min(250 , max(f_spped_ice* 250, f_spped_snow * 250))\n",
    "        \n",
    "        #function correlating snow accumulation depth to accident probability\n",
    "        f_accident_snow = 0\n",
    "        if snow_convert <= 0.1: f_accident_snow = 0\n",
    "        elif snow_convert <5 and  snow_convert > 0.1: f_accident_snow = (snow_convert - 0.1)/(4.9)\n",
    "        elif snow_convert >= 0.5: f_accident_snow = 1\n",
    "        \n",
    "        f_accident_ice = 0\n",
    "        if ice_convert <= 0.05: f_accident_ice = 0\n",
    "        elif ice_convert > 0.05 and ice_convert< 0.5 : f_accident_ice = (ice_convert-0.05)/(0.45)\n",
    "        elif ice_convert >=0.5 : f_accident_ice = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        # â€¢\tCost accident is the cost associated with accidents\n",
    "        Cost_accident = f_accident_snow * 25 + f_accident_ice * 17\n",
    "            \n",
    "            \n",
    "#         print (\"Cost_accident\", Cost_accident)  \n",
    "#         print (\"Salt\", salt)\n",
    "         \n",
    "        reward = - cost - Cost_speed - Cost_accident\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642adbaef044a0f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-11T15:58:14.983545500Z",
     "start_time": "2024-03-11T15:57:51.732984800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "{'state': [-2.47, -6.74, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-4.38, -7.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cumulative_reward': 0, 'done': False}\n",
      "{'state': [-4.38, -7.44, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-4.62, -6.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cumulative_reward': 0, 'done': False}\n",
      "{'state': [-4.62, -6.04, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-3.95, -5.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cumulative_reward': 0, 'done': False}\n",
      "{'state': [-3.95, -5.84, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-0.2, 2.66, 0.0, 0.00457, 0.0, 0.0, 0.0, 0.0], 'cumulative_reward': 0, 'done': False}\n",
      "{'state': [-0.2, 2.66, 0.0, 0.00457, 0.0, 0.0, 0.0, 0.0], 'action': 2, 'reward': -25.293, 'next_state': [3.27, 3.76, 0.0, 0.00762, 0.04113, 0.0, 189.9, 0.0], 'cumulative_reward': -25.293, 'done': False}\n",
      "{'state': [3.27, 3.76, 0.0, 0.00762, 0.04113, 0.0, 189.9, 0.0], 'action': 0, 'reward': 0, 'next_state': [2.15, -4.34, 0.0, 0.01041, 0.0, 0.3942299042570355, 170.91, 0.0], 'cumulative_reward': -25.293, 'done': False}\n",
      "{'state': [2.15, -4.34, 0.0, 0.01041, 0.0, 0.3942299042570355, 170.91, 0.0], 'action': 0, 'reward': 0, 'next_state': [-1.46, -6.14, 0.51, 0.01346, 0.0, 0.4981255466009555, 153.819, 0.0], 'cumulative_reward': -25.293, 'done': False}\n",
      "{'state': [-1.46, -6.14, 0.51, 0.01346, 0.0, 0.4981255466009555, 153.819, 0.0], 'action': 0, 'reward': 0, 'next_state': [-3.06, -7.34, 1.06, 0.00051, 0.0, 21.861833782128034, 138.4371, 0.0], 'cumulative_reward': -25.293, 'done': False}\n",
      "{'state': [-3.06, -7.34, 1.06, 0.00051, 0.0, 21.861833782128034, 138.4371, 0.0], 'action': 0, 'reward': -258.57240342463314, 'next_state': [-1.68, -7.44, 0.82, 0.0, 0.0459, 0.0, 135.66835799999998, 4.1537484186043265], 'cumulative_reward': -283.86540342463314, 'done': False}\n",
      "{'state': [-1.68, -7.44, 0.82, 0.0, 0.0459, 0.0, 135.66835799999998, 4.1537484186043265], 'action': 0, 'reward': -69.51132188388763, 'next_state': [-1.68, -7.44, 0.82, 0.0, 0.0459, 0.0, 132.95499084, 0.8316676837208654], 'cumulative_reward': -353.3767253085208, 'done': True}\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "{'state': [-19.12, -22.87, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-20.19, -21.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'cumulative_reward': 0, 'done': False}\n",
      "{'state': [-20.19, -21.17, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], 'action': 2, 'reward': -39.44, 'next_state': [-20.27, -20.27, 0.39, 0.0, 0.0, 0.0, 392.0, 0.0], 'cumulative_reward': -39.44, 'done': False}\n",
      "{'state': [-20.27, -20.27, 0.39, 0.0, 0.0, 0.0, 392.0, 0.0], 'action': 0, 'reward': 0, 'next_state': [-18.4, -20.67, 0.0, 0.0, 0.0, 0.0, 384.15999999999997, 0.0], 'cumulative_reward': -39.44, 'done': False}\n",
      "{'state': [-18.4, -20.67, 0.0, 0.0, 0.0, 0.0, 384.15999999999997, 0.0], 'action': 0, 'reward': 0, 'next_state': [-13.94, -11.77, 0.0, 0.00457, 0.0, 0.0, 376.47679999999997, 0.0], 'cumulative_reward': -39.44, 'done': False}\n",
      "{'state': [-13.94, -11.77, 0.0, 0.00457, 0.0, 0.0, 376.47679999999997, 0.0], 'action': 0, 'reward': -250.3291836734694, 'next_state': [-12.08, -15.37, 0.0, 0.00584, 0.4113, 0.0, 368.94726399999996, 0.0], 'cumulative_reward': -289.7691836734694, 'done': False}\n",
      "{'state': [-12.08, -15.37, 0.0, 0.00584, 0.4113, 0.0, 368.94726399999996, 0.0], 'action': 0, 'reward': -251.4018367346939, 'next_state': [-14.78, -21.37, 0.0, 0.00711, 0.9369, 0.0, 361.56831872, 0.008226], 'cumulative_reward': -541.1710204081633, 'done': False}\n",
      "{'state': [-14.78, -21.37, 0.0, 0.00711, 0.9369, 0.0, 361.56831872, 0.008226], 'action': 0, 'reward': -252.7077551020408, 'next_state': [-18.18, -22.37, 0.0, 0.00813, 1.5768, 0.0, 354.33695234559997, 0.0203832], 'cumulative_reward': -793.878775510204, 'done': False}\n",
      "{'state': [-18.18, -22.37, 0.0, 0.00813, 1.5768, 0.0, 354.33695234559997, 0.0203832], 'action': 0, 'reward': -254.20102040816326, 'next_state': [-18.23, -23.07, 0.71, 0.0, 2.3085, 0.0, 347.250213298688, 0.03561264], 'cumulative_reward': -1048.0797959183674, 'done': False}\n",
      "{'state': [-18.23, -23.07, 0.71, 0.0, 2.3085, 0.0, 347.250213298688, 0.03561264], 'action': 0, 'reward': -254.20102040816326, 'next_state': [-17.8, -23.37, 0.57, 0.0, 2.3085, 0.0, 340.30520903271423, 0.053292528], 'cumulative_reward': -1302.2808163265306, 'done': False}\n",
      "{'state': [-17.8, -23.37, 0.57, 0.0, 2.3085, 0.0, 340.30520903271423, 0.053292528], 'action': 0, 'reward': -254.20102040816326, 'next_state': [-17.8, -23.37, 0.57, 0.0, 2.3085, 0.0, 333.49910485205993, 0.0568285056], 'cumulative_reward': -1556.4818367346938, 'done': True}\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Assuming the DQNAgent and RoadEnv classes are defined as in your previous code\n",
    "\n",
    "# Load and preprocess dataset\n",
    "file_path = 'Modified_DataSetV8_ConvertedUnits.csv'\n",
    "dataset = pd.read_csv(file_path)\n",
    "for col in ['initial_Snow', 'initial_water', 'initial_Salt', 'initial_ice']:\n",
    "    if col not in dataset.columns:\n",
    "        dataset[col] = 0\n",
    "\n",
    "# Initialize environment and agent\n",
    "\n",
    "\n",
    "action_size = 4\n",
    "no_intervention_data_size = 0#10 * 4  # 10 future states * 4 variables per state\n",
    "current_state_size = 8  # Current state variables\n",
    "total_state_size = current_state_size + no_intervention_data_size\n",
    "\n",
    "#agent = DQNAgent(total_state_size, action_size)\n",
    "agent = DQNAgent(total_state_size, action_size, model_path='initial_55k_60k_Condor.h5')\n",
    "global_episode_num = 0 \n",
    "\n",
    "# Split dataset into training and validation\n",
    "unique_window_ids = dataset['Window_ID'].unique()\n",
    "env = RoadEnv(dataset, unique_window_ids)\n",
    "total_training_windows = len(unique_window_ids)\n",
    "print(f\"Total training window IDs: {total_training_windows}\")\n",
    "\n",
    "# # Use only the first 100 episodes for training, excluding the last 10 for validation\n",
    "# train_window_ids = unique_window_ids[:100]  # Adjusted to use only the first 100 episodes\n",
    "validation_window_ids = unique_window_ids[-20:]  # Last 10 for validation\n",
    "\n",
    "# Current_training_windows = len(train_window_ids)\n",
    "# print(f\"Current training window IDs: {Current_training_windows}\")\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Log file path\n",
    "log_file_path = 'validation_log_simpleModel_Trial_2.txt'\n",
    "\n",
    "# Training\n",
    "batch_size = 16  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "def run_episode_simulation(agent, current_episode_data, state_size, episode_num):\n",
    "    best_trial_reward = -np.inf\n",
    "    best_trial_data = None\n",
    "\n",
    "    # Initial simulation setup\n",
    "    #for trial in range(N_TRIALS):\n",
    "        # print(f\"Trial {trial}\")\n",
    "    trial_data = {'states': [], 'actions': [], 'rewards': [], 'next_states': [], 'dones': []}\n",
    "    state = env.reset(episode_num)\n",
    "        # print (\"state\",state)\n",
    "        #cumulative_reward = 0\n",
    "        # print (\"current_episode_data\",current_episode_data)\n",
    "    i = 0\n",
    "    for index, row in current_episode_data.iterrows():\n",
    "        state = np.reshape(state, [1, total_state_size])\n",
    "        # print (\"state\", state)\n",
    "        if  i == 0 or i ==1:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.act(state)\n",
    "        i +=1\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # print (\"next_state\",next_state)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        # # Save trial data\n",
    "        trial_data['states'].append(state)\n",
    "        trial_data['actions'].append(action)\n",
    "        trial_data['rewards'].append(reward)\n",
    "        trial_data['next_states'].append(next_state)\n",
    "        trial_data['dones'].append(done)\n",
    "\n",
    "        state = next_state\n",
    "        # cumulative_reward += reward\n",
    "\n",
    "        # if cumulative_reward > best_trial_reward:\n",
    "        #     best_trial_reward = cumulative_reward\n",
    "    best_trial_data = trial_data\n",
    "        #     # print (\"cumulative_reward\", cumulative_reward)\n",
    "        #     # print (\"best_trial_reward\", best_trial_reward)\n",
    "        # # print (\"best_trial_data\", best_trial_data)\n",
    "    return best_trial_data\n",
    "print (\"len(unique_window_ids)\", len(unique_window_ids))\n",
    "N_TRIALS = 2000  # Number of trials to run for each episode\n",
    "for batch_start in range(global_episode_num, len(unique_window_ids), N_TRIALS):\n",
    "    batch_end = min(batch_start + N_TRIALS, len(unique_window_ids))\n",
    "    print (f\"batch_start {batch_start}, batch_end {batch_end}\")\n",
    "    train_window_ids = unique_window_ids[batch_start:batch_end]\n",
    "    \n",
    "    for i, window_id in enumerate(train_window_ids):\n",
    "        print (\"global_episode_num\", global_episode_num)\n",
    "        Current_training_windows = len(train_window_ids)\n",
    "        print(f\"****************************************Processing Window ID {window_id} ({i+1}/{Current_training_windows})\")\n",
    "        current_episode_data = dataset[dataset['Window_ID'] == window_id]\n",
    "        # print (\"current_episode_data\", current_episode_data)\n",
    "        # Run episode simulation and get the best trial data\n",
    "        # print (\"iiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\", i)\n",
    "        best_trial_data = run_episode_simulation(agent, current_episode_data, total_state_size, episode_num = global_episode_num)\n",
    "        #Adjust learning rate\n",
    "        #agent.adjust_learning_rate(global_episode_num)\n",
    "\n",
    "        # Increment the global episode counter after processing each episode\n",
    "        global_episode_num += 1\n",
    "        # Store the best trial's outcomes in memory\n",
    "        for state, action, reward, next_state, done in zip(best_trial_data['states'], best_trial_data['actions'], best_trial_data['rewards'], best_trial_data['next_states'], best_trial_data['dones']):\n",
    "            #print (f\" state= {state} .., action={action} .., reward={reward} .., next_state = {next_state} ..,done={done}\")\n",
    "            #print (f\" action={action} .., reward={reward}\")\n",
    "        \n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # Clear the output\n",
    "            clear_output(wait=True)\n",
    "        print (\"len(agent.memory)\", len(agent.memory), \"batch_size\", batch_size)\n",
    "\n",
    "        if len(agent.memory) % batch_size ==0:\n",
    "            agent.replay(batch_size)\n",
    "\n",
    "    # Clear the output\n",
    "    clear_output(wait=True)\n",
    "    # Save the trained model\n",
    "    model_name = f'models/Simple_dqn_model_episodes_{batch_start}_{batch_end}.h5'\n",
    "    agent.model.save(model_name)\n",
    "    validation_episode_Number = 43057\n",
    "    with open(log_file_path, 'a') as log_file:\n",
    "        log_file.write(f\"Validation for episodes {batch_start} to {batch_end} , # Exploration: {agent.Number_ofExploration}, # Exploitation: {agent.Number_ofExploitation}  \\n\")\n",
    "        for window_id in validation_window_ids:\n",
    "            current_episode_data = dataset[dataset['Window_ID'] == window_id]\n",
    "            state = env.reset(validation_episode_Number)  # Reset environment and action counters\n",
    "            validation_episode_Number +=1\n",
    "            episode_actions = []\n",
    "            episode_info = []\n",
    "            cumulative_reward = 0\n",
    "\n",
    "            for index, row in current_episode_data.iterrows():\n",
    "                state = np.reshape(state, [1, total_state_size])\n",
    "\n",
    "                # Use predict_action to exploit the learned policy\n",
    "                action = agent.predict_action(state)\n",
    "\n",
    "                next_state, reward, done = env.step(action)\n",
    "                next_state = np.reshape(next_state, [1, total_state_size])\n",
    "                cumulative_reward += reward\n",
    "\n",
    "                episode_actions.append(action)\n",
    "                episode_info.append({\n",
    "                    'state': state.flatten().tolist(),\n",
    "                    'action': action,\n",
    "                    'reward': reward,\n",
    "                    'next_state': next_state.flatten().tolist(),\n",
    "                    'cumulative_reward': cumulative_reward,\n",
    "                    'done': done\n",
    "                })\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # print(f\"\\nEpisode for Window ID: {window_id}\")\n",
    "            # print(\"Actions taken:\", episode_actions)\n",
    "            # print(\"Cumulative Reward:\", cumulative_reward)\n",
    "            for info in episode_info:\n",
    "                print(info)\n",
    "            # Log the actions for each episode\n",
    "            log_file.write(f\"Episode for Window ID: {window_id}, Actions taken: {episode_actions}, Cumulative Reward: {cumulative_reward}\\n\")\n",
    "\n",
    "        print(\"Training and validation complete for this batch.\")\n",
    "print (\"Training complete\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
